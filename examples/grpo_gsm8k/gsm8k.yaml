data:
  # basic info
  dataset_path: 'openai/gsm8k'
  subset_name: "main"
  train_split: 'train'
  eval_split: 'test'
  format_config:
    prompt_key: 'question'
    response_key: 'answer'
  # data active iterator related
  dj_process_desc: 'Please compute difficulty scores for these math questions.'
  agent_model_name: 'qwen-max'
  agent_model_config:
    config_name: 'my-qwen-instruction'
    model_type: 'dashscope_chat'
    model_name: 'qwen2.5-72b-instruct'
  clean_strategy: 'iterative'
  # db related
  db_url: ''
  # downstream loading related
  total_epochs: 1
  batch_size: 96
  default_workflow_type: 'math_workflow'
model:
  model_path: '/PATH/TO/MODEL/'
  max_prompt_tokens: 256
  max_response_tokens: 1024
  checkpoint_path: ""
cluster:
  node_num: 1
  gpu_per_node: 8
buffer:
  max_retry_times: 3
  max_retry_interval: 1
  train_dataset:
    name: gsm8k_buffer
    storage_type: queue
    path: 'sqlite:///gsm8k.db'
  # sft_warmup_dataset: # Uncomment these to enable sft warmup
  #   name: warmup_data
  #   storage_type: file
  #   path: '/PATH/TO/WARMUP_DATA/'
  #   kwargs:
  #     prompt_type: plaintext
explorer:
  engine_type: vllm_async
  engine_num: 2
  runner_num: 32
  tensor_parallel_size: 1
  enable_prefix_caching: false
  enforce_eager: true
  dtype: bfloat16
  temperature: 1.0
  seed: 42
  logprobs: 0
  repeat_times: 8
  use_ray: false
  backend: 'nccl'
  max_pending_requests: 32
  max_waiting_steps: 4
synchronizer:
  sync_method: 'nccl'
  sync_iteration_interval: 2
  sync_timeout: 1200
trainer:
  trainer_type: 'verl'
  algorithm_type: ppo
  trainer_config_path: 'examples/grpo_gsm8k/train_gsm8k.yaml'
  sft_warmup_iteration: 0 # Set to integer to enable sft warmup
  eval_interval: 50
  save_interval: 100
  # get_exp_strategy: 'LFU'
monitor:
  cache_root_dir: ""
  project: "Trinity-RFT-gsm8k"
  name: "qwen2.5-1.5B-gsm8k"
