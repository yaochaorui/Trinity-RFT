

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Megatron-LM Backend &mdash; Trinity-RFT 0.2.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=37f418d5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Asynchronous RFT" href="example_async_mode.html" />
    <link rel="prev" title="Off-Policy RFT" href="example_reasoning_advanced.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Trinity-RFT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="example_reasoning_basic.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_reasoning_advanced.html">Off-Policy RFT</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Megatron-LM Backend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-1-installation">Step 1: Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#minimum-requirements">Minimum Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-dependencies">Install Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#alternative-use-docker">Alternative: Use Docker</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-configure-and-run-training">Step 2: Configure and Run Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#megatron-configuration-example">Megatron Configuration Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-mixture-of-experts-moe-models">Training Mixture-of-Experts (MoE) Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="example_async_mode.html">Asynchronous RFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_multi_turn.html">Concatenated Multi-Turn RFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_step_wise.html">General Multi-Step RFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_react.html">Multi-Step ReAct</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_search_email.html">Email Search Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_dpo.html">Offline DPO and SFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_data_functionalities.html">Data Processing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guidelines</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="trinity_programming_guide.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="trinity_configs.html">Configuration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_mix_algo.html">Algorithm Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="synchronizer.html">Synchronizer in Trinity-RFT</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Trinity-RFT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Megatron-LM Backend</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/modelscope/Trinity-RFT/blob/main/docs/sphinx_doc/source/tutorial/example_megatron.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="megatron-lm-backend">
<h1>Megatron-LM Backend<a class="headerlink" href="#megatron-lm-backend" title="Link to this heading"></a></h1>
<p>This guide walks you through how to train models using <strong>Megatron-LM</strong> in a clear way.</p>
<hr class="docutils" />
<section id="step-1-installation">
<h2>Step 1: Installation<a class="headerlink" href="#step-1-installation" title="Link to this heading"></a></h2>
<section id="minimum-requirements">
<h3>Minimum Requirements<a class="headerlink" href="#minimum-requirements" title="Link to this heading"></a></h3>
<p>Before you begin, make sure your system meets these requirements:</p>
<ul class="simple">
<li><p><strong>GPUs</strong>: At least 2 GPUs (for distributed training)</p></li>
<li><p><strong>CUDA</strong>: Version 12.4 or higher</p></li>
<li><p><strong>Python</strong>: Version 3.10 or higher</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="install-dependencies">
<h3>Install Dependencies<a class="headerlink" href="#install-dependencies" title="Link to this heading"></a></h3>
<p>Start by cloning the repository and setting up a virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Clone the repository</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/modelscope/Trinity-RFT
<span class="nb">cd</span><span class="w"> </span>Trinity-RFT
</pre></div>
</div>
<section id="option-a-using-conda">
<h4>Option A: Using Conda<a class="headerlink" href="#option-a-using-conda" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and activate a new environment</span>
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>trinity<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10
conda<span class="w"> </span>activate<span class="w"> </span>trinity
</pre></div>
</div>
</section>
<section id="option-b-using-venv">
<h4>Option B: Using venv<a class="headerlink" href="#option-b-using-venv" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and activate a virtual environment</span>
python3.10<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>.venv
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate
</pre></div>
</div>
</section>
<section id="install-the-package">
<h4>Install the Package<a class="headerlink" href="#install-the-package" title="Link to this heading"></a></h4>
<p>Install the project in editable mode with Megatron support:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># For bash users</span>
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="o">[</span>megatron<span class="o">]</span>

<span class="c1"># For zsh users (escape the brackets)</span>
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="se">\[</span>megatron<span class="se">\]</span>
</pre></div>
</div>
</section>
<section id="install-flash-attention">
<h4>Install Flash Attention<a class="headerlink" href="#install-flash-attention" title="Link to this heading"></a></h4>
<p>After installing the base dependencies, install <code class="docutils literal notranslate"><span class="pre">flash-attn</span></code>. This may take several minutes to compile — please be patient!</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>flash-attn<span class="o">==</span><span class="m">2</span>.8.1<span class="w"> </span>-v
</pre></div>
</div>
<p>If you run into installation issues, try this alternative:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>flash-attn<span class="w"> </span>-v<span class="w"> </span>--no-build-isolation
</pre></div>
</div>
</section>
<section id="install-apex-from-nvidia">
<h4>Install Apex (from NVIDIA)<a class="headerlink" href="#install-apex-from-nvidia" title="Link to this heading"></a></h4>
<p>Finally, install NVIDIA’s Apex library for mixed-precision training:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>--disable-pip-version-check<span class="w"> </span>--no-cache-dir<span class="w"> </span>--no-build-isolation<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--config-settings<span class="w"> </span><span class="s2">&quot;--build-option=--cpp_ext&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--config-settings<span class="w"> </span><span class="s2">&quot;--build-option=--cuda_ext&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--resume-retries<span class="w"> </span><span class="m">999</span><span class="w"> </span>git+https://github.com/NVIDIA/apex.git
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="alternative-use-docker">
<h3>Alternative: Use Docker<a class="headerlink" href="#alternative-use-docker" title="Link to this heading"></a></h3>
<p>We provide a Docker setup to simplify environment management.</p>
<section id="build-the-docker-image">
<h4>Build the Docker Image<a class="headerlink" href="#build-the-docker-image" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/modelscope/Trinity-RFT
<span class="nb">cd</span><span class="w"> </span>Trinity-RFT

<span class="c1"># Build the image</span>
docker<span class="w"> </span>build<span class="w"> </span>-f<span class="w"> </span>scripts/docker_for_megatron/Dockerfile<span class="w"> </span>-t<span class="w"> </span>trinity-rft-megatron:latest<span class="w"> </span>.
</pre></div>
</div>
<blockquote>
<div><p>💡 You can customize the Dockerfile before building — for example, to add pip mirrors or set API keys.</p>
</div></blockquote>
</section>
<section id="run-the-container">
<h4>Run the Container<a class="headerlink" href="#run-the-container" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="o">=</span><span class="s2">&quot;64g&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--rm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="nv">$PWD</span>:/workspace<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>&lt;your_data_and_checkpoints_path&gt;:/data<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trinity-rft-megatron:latest
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">&lt;your_data_and_checkpoints_path&gt;</span></code> with the actual path on your machine where datasets and model checkpoints are stored.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="step-2-configure-and-run-training">
<h2>Step 2: Configure and Run Training<a class="headerlink" href="#step-2-configure-and-run-training" title="Link to this heading"></a></h2>
<p>Most configuration settings are covered in the <a class="reference internal" href="#../../docs/sphinx_doc/source/tutorial/example_reasoning_basic.md#step-0-environment-preparation"><span class="xref myst">Quick Start Guide</span></a>. Here, we’ll focus only on <strong>Megatron-LM-specific</strong> settings.</p>
<section id="megatron-configuration-example">
<h3>Megatron Configuration Example<a class="headerlink" href="#megatron-configuration-example" title="Link to this heading"></a></h3>
<p>Below is an example of how to configure the actor, reference model, and critic to use Megatron-LM:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">actor_rollout_ref</span><span class="p">:</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">actor</span><span class="p p-Indicator">:</span>
<span class="w">    </span><span class="nt">strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">megatron</span><span class="w">  </span><span class="c1"># Kept for backward compatibility</span>
<span class="w">    </span><span class="nt">megatron</span><span class="p">:</span>
<span class="w">      </span><span class="c1"># Model parallelism settings</span>
<span class="w">      </span><span class="nt">tensor_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">      </span><span class="nt">pipeline_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">expert_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>

<span class="w">      </span><span class="c1"># Offloading (set to false unless you&#39;re memory-constrained)</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">grad_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">optimizer_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="w">      </span><span class="c1"># Use mBridge for parameter import/export (optional)</span>
<span class="w">      </span><span class="nt">use_mbridge</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="w">      </span><span class="c1"># Recomputation settings (helps save memory during training)</span>
<span class="w">      </span><span class="nt">override_transformer_config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">recompute_granularity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">full</span>
<span class="w">        </span><span class="nt">recompute_method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">        </span><span class="nt">recompute_num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
<span class="w">  </span><span class="nt">ref</span><span class="p">:</span>
<span class="w">    </span><span class="nt">megatron</span><span class="p">:</span>
<span class="w">      </span><span class="nt">tensor_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">      </span><span class="nt">pipeline_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">expert_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">grad_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">optimizer_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">use_mbridge</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">      </span><span class="nt">override_transformer_config</span><span class="p">:</span>
<span class="w">        </span><span class="nt">recompute_granularity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">full</span>
<span class="w">        </span><span class="nt">recompute_method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">        </span><span class="nt">recompute_num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>

<span class="nt">critic</span><span class="p">:</span>
<span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">megatron</span>
<span class="w">  </span><span class="nt">megatron</span><span class="p">:</span>
<span class="w">    </span><span class="nt">tensor_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">    </span><span class="nt">pipeline_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">expert_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">grad_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">optimizer_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">use_mbridge</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">override_transformer_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">recompute_granularity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">full</span>
<span class="w">      </span><span class="nt">recompute_method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uniform</span>
<span class="w">      </span><span class="nt">recompute_num_layers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="training-mixture-of-experts-moe-models">
<h3>Training Mixture-of-Experts (MoE) Models<a class="headerlink" href="#training-mixture-of-experts-moe-models" title="Link to this heading"></a></h3>
<p>If you’re training an MoE model like <strong>Qwen/Qwen3-30B-A3B</strong>, you have two options:</p>
<ol class="arabic simple">
<li><p><strong>Enable mBridge</strong>: Set <code class="docutils literal notranslate"><span class="pre">use_mbridge:</span> <span class="pre">true</span></code> in the config.</p></li>
<li><p><strong>Convert the model first</strong>: Use the <a class="reference external" href="https://github.com/volcengine/verl/blob/main/scripts/converter_hf_to_mcore.py">Hugging Face to MCore converter</a> from the <strong>verl</strong> to convert your model before training.</p></li>
</ol>
<blockquote>
<div><p>⚠️ Without one of these steps, MoE models may not load or train correctly.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="example_reasoning_advanced.html" class="btn btn-neutral float-left" title="Off-Policy RFT" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="example_async_mode.html" class="btn btn-neutral float-right" title="Asynchronous RFT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Trinity-RFT Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    <b>main</b>
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><b><a href="example_megatron.html">main</a> (latest)</b></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>