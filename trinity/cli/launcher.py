"""Launch the trainer"""
import argparse
import os
import sys
from pathlib import Path
from pprint import pprint

import ray

from trinity.common.config import Config, load_config
from trinity.common.constants import AlgorithmType
from trinity.explorer.explorer import Explorer
from trinity.trainer.trainer import Trainer
from trinity.utils.log import get_logger

logger = get_logger(__name__)


def bench(config: Config) -> None:
    """Evaluate model."""
    explorer = Explorer.remote(config)
    try:
        ray.get(explorer.prepare.remote())
        ray.get(explorer.benchmark.remote())
        logger.info("Benchmark finished.")
        ray.get(explorer.shutdown.remote())
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        raise e


def explore(config: Config) -> None:
    """Run explorer."""
    explorer = Explorer.remote(config)
    try:
        ray.get(explorer.prepare.remote())
        ray.get(explorer.sync_weight.remote())
        ray.get(explorer.explore.remote())
        logger.info("Explore finished.")
        ray.get(explorer.shutdown.remote())
    except Exception as e:
        logger.error(f"Explore failed: {e}")
        raise e


def train(config: Config) -> None:
    """Run trainer."""

    trainer = Trainer.remote(config)
    ray.get(trainer.prepare.remote())

    if config.buffer.trainer_input.sft_warmup_steps > 0:
        while True:
            train_continue, train_step_num = ray.get(
                trainer.train_one_period.remote(AlgorithmType.SFT)
            )
            if train_step_num <= config.buffer.trainer_input.sft_warmup_steps:
                logger.info(f"SFT warmup step {train_step_num} finished.")
            if not train_continue:
                logger.info("SFT warmup finished.")
                break

    algo_type = config.algorithm.algorithm_type
    try:
        ray.get(trainer.train.remote(algo_type))
        logger.info("Train finished.")
        ray.get(trainer.shutdown.remote())
    except Exception as e:
        logger.error(f"Train failed {e}.")
        raise e


def both(config: Config) -> None:
    """Setup both explorer and trainer.

    For the explorer, a step contains `batch_size * sync_interval` number
    of rollout tasks.

    For the trainer, it has to consume all experiences generated by the explorer in
    the latest step. The specific number of experiences may vary for different
    algorithms and tasks.
    """
    explorer = Explorer.remote(config)
    trainer = Trainer.remote(config)
    ray.get([explorer.__ray_ready__.remote(), trainer.__ray_ready__.remote()])
    logger.info("Setup explorer and trainer finished.")
    ray.get(
        [
            explorer.prepare.remote(),
            trainer.prepare.remote(),
        ]
    )
    # sync weight before training start
    ray.get([explorer.sync_weight.remote(), trainer.sync_weight.remote()])

    if config.buffer.trainer_input.sft_warmup_steps > 0:
        while True:
            train_continue, train_step_num = ray.get(
                trainer.train_one_period.remote(AlgorithmType.SFT)
            )
            if train_step_num <= config.buffer.trainer_input.sft_warmup_steps:
                logger.info(f"SFT warmup step {train_step_num} finished.")
            if not train_continue:
                logger.info("SFT warmup finished.")
                break
        ray.get([explorer.sync_weight.remote(), trainer.sync_weight.remote()])

    algo_type = config.algorithm.algorithm_type
    while True:
        try:
            ref_explore = explorer.explore_one_period.remote()
            ref_train = trainer.train_one_period.remote(algo_type)
            explore_continue, explore_step_num = ray.get(ref_explore)
            train_continue, train_step_num = ray.get(ref_train)
            if not explore_continue:
                # If explore finished, the trainer may not have enough experiences to continue,
                # which will cause the trainer be blocked. So we stop the training process
                # immediately.
                # TODO: use a more elegant way to stop the training process.
                logger.info("Explorer finished, stopping...")
                break
            if not train_continue:
                logger.info("Trainer finished, stopping...")
                break
            ray.get([explorer.sync_weight.remote(), trainer.sync_weight.remote()])
            logger.info("Model weight synchronized.")
        except Exception as e:
            logger.error(e)
            logger.error("Training stopped due to exception.")
            raise e
        if explore_step_num % config.explorer.eval_interval == 0:
            try:
                ray.get(explorer.eval.remote())
                logger.info("Evaluation finished.")
            except Exception as e:
                logger.error(e)
                logger.error("Evaluation failed.")
                raise e
        ray.get(explorer.flush_log.remote(step=explore_step_num))
        ray.get(trainer.flush_log.remote(step=train_step_num))

    ray.get(explorer.shutdown.remote())
    ray.get(trainer.shutdown.remote())


def activate_data_module(data_workflow_url: str, config_path: str):
    """Check whether to activate data module and preprocess datasets."""
    from trinity.cli.client import request

    logger.info("Activating data module...")
    res = request(
        url=data_workflow_url,
        configPath=config_path,
    )
    if res["return_code"] != 0:
        logger.error(f"Failed to activate data module: {res['return_msg']}.")
        return


def run(config_path: str, dlc: bool = False):
    config = load_config(config_path)
    config.check_and_update()
    pprint(config)
    # try to activate data module
    data_processor_config = config.data_processor
    if data_processor_config.data_workflow_url and (
        data_processor_config.dj_config_path or data_processor_config.dj_process_desc
    ):
        activate_data_module(data_processor_config.data_workflow_url, config_path)
    ray_namespace = f"{config.project}-{config.name}"
    if dlc:
        from trinity.utils.dlc_utils import setup_ray_cluster

        setup_ray_cluster(namespace=ray_namespace)
    else:
        from trinity.utils.dlc_utils import is_running

        if not is_running:
            raise RuntimeError("Ray is not running, please start it by `ray start --head`.")
        ray.init(namespace=ray_namespace, ignore_reinit_error=True)
    if config.mode == "explore":
        explore(config)
    elif config.mode == "train":
        train(config)
    elif config.mode == "both":
        both(config)
    elif config.mode == "bench":
        bench(config)


def studio(port: int = 8501):
    from streamlit.web import cli as stcli

    current_dir = Path(__file__).resolve().parent.parent
    config_manager_path = os.path.join(current_dir, "manager", "config_manager.py")

    sys.argv = [
        "streamlit",
        "run",
        config_manager_path,
        "--server.port",
        str(port),
        "--server.fileWatcherType",
        "none",
    ]
    sys.exit(stcli.main())


def main() -> None:
    """The main entrypoint."""
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command", required=True)

    # run command
    run_parser = subparsers.add_parser("run", help="Run RFT process.")
    run_parser.add_argument("--config", type=str, required=True, help="Path to the config file.")
    run_parser.add_argument(
        "--dlc", action="store_true", help="Specify when running in Aliyun PAI DLC."
    )

    # studio command
    studio_parser = subparsers.add_parser("studio", help="Run studio.")
    studio_parser.add_argument(
        "--port", type=int, default=8501, help="The port for Trinity-Studio."
    )

    # TODO: add more commands like `monitor`, `label`

    args = parser.parse_args()
    if args.command == "run":
        # TODO: support parse all args from command line
        run(args.config, args.dlc)
    elif args.command == "studio":
        studio(args.port)


if __name__ == "__main__":
    main()
